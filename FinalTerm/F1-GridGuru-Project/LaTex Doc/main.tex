\documentclass{article}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts,amsthm, mathrsfs}
\usepackage[title]{appendix}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot, booktabs}
\usepackage{algorithm, algorithmicx, algpseudocode}
\usepackage{listings}
\usepackage{biblatex}

\usepackage[english]{babel}

\raggedbottom

\lstset{
    language=Python,                 
    basicstyle=\ttfamily\small,      
    keywordstyle=\color{blue},       
    stringstyle=\color{red},         
    commentstyle=\color{orange},      
    morecomment=[s][\color{gray}]{"""}{"""}, 
    numbers=left,                    
    numberstyle=\tiny\color{gray},   
    stepnumber=1,                    
    numbersep=10pt,                  
    backgroundcolor=\color{white},   
    showspaces=false,                
    showstringspaces=false,          
    showtabs=false,                  
    frame=single,                    
    captionpos=b,                    
    breaklines=true,                 
    breakatwhitespace=false,         
    tabsize=4,                       
    escapeinside={\%*}{*}           
}

\begin{document}

\title{F1 EDA - Big Data Final Term Project}

\author{Enrique Ulises Báez Gómez Tagle \\ Sara Rocío Miranda Mateos}

\maketitle

% MISSING \begin{abstract}% MISSING 

% MISSING \end{abstract}

\tableofcontents
\newpage

% MISSING \section{Introduction}% MISSING 


\section{Infrastructure / Architecture}

\begin{itemize}
    \item \textbf{Data Lakehouse Configuration:} Central to our project is a Data Lakehouse architecture designed for scalability and resilience within an AWS environment. This setup includes multiple layers of data storage and processing capabilities, ensuring efficient data management and analysis.
    
    \item \textbf{Storage Layers:} 
        \begin{itemize}
            \item \textbf{Raw Data Storage (RAW Layer):} Utilizes Amazon S3 to store unprocessed, raw data ingested from various sources.
            \item \textbf{Cleansed Data Storage (Cleansed Layer):} Stores cleaned and transformed data in Amazon S3, ready for further analysis.
            \item \textbf{Data Warehouse Storage (Data Warehouse Layer):} Contains structured and optimized data in Amazon S3, prepared for querying and visualization.
        \end{itemize}
    
    \item \textbf{Data Ingestion:} 
        \begin{itemize}
            \item \textbf{Real-Time Data Ingestion (AWS Lambda):} Handles the real-time ingestion of data, storing it in the RAW Layer.
            \item \textbf{Batch Data Ingestion (Static Dataset):} Manages the batch ingestion of static datasets, such as CSV files, into the RAW Layer.
        \end{itemize}
    
    \item \textbf{Data Transformation:} 
        \begin{itemize}
            \item \textbf{ETL Processing Jobs (AWS Glue):} Performs ETL (Extract, Transform, Load) operations to clean and transform data from the RAW Layer to the Cleansed Layer.
        \end{itemize}
    
    \item \textbf{Data Analysis:} 
        \begin{itemize}
            \item \textbf{Data Analytics and Processing (AWS EMR):} Utilizes Amazon EMR to run advanced data processing and analytical tasks on data from the Cleansed Layer, producing structured results stored in the Data Warehouse Layer.
        \end{itemize}
    
    \item \textbf{Data Cataloging:} 
        \begin{itemize}
            \item \textbf{Metadata Management (AWS Glue Data Catalog):} Catalogs the metadata of datasets in the Cleansed Layer, facilitating efficient data discovery and access.
        \end{itemize}
    
    \item \textbf{Data Visualization:} 
        \begin{itemize}
            \item \textbf{Visualization and Analysis (RStudio):} Processes the curated datasets in the Data Warehouse Layer locally to create insightful visualizations and analysis, supporting decision-making.
        \end{itemize}
    
    \item \textbf{Deployment Process:}
        \begin{enumerate}
            \item We initiated our deployment by setting up the AWS CLI, enabling us to interact with AWS services seamlessly.
            \item Amazon S3 buckets were created to host our data layers, carefully chosen to be in proximity to our compute resources to minimize latency.
            \item For infrastructure provisioning, we utilized both AWS CloudFormation and Terraform. AWS CloudFormation was used for managing AWS-specific resources, ensuring consistent and repeatable deployments within the AWS ecosystem. Terraform was employed for managing multi-cloud resources and ensuring infrastructure as code principles across various environments.
            \item AWS Glue was configured to automate our ETL processes, facilitating efficient data transformation and loading into our cleansed and data warehouse layers.
            \item Amazon EMR clusters were set up for big data processing, with configurations tailored to optimize performance and cost.
            \item Continuous Integration and Continuous Deployment (CI/CD) pipelines were implemented using AWS CodePipeline and Jenkins, automating the deployment process and ensuring that updates to our infrastructure and applications could be rolled out smoothly.
            \item Monitoring and logging were established using Amazon CloudWatch and AWS CloudTrail, providing real-time insights and audit trails for all activities within our environment.
            \item Security best practices were followed, including the use of IAM roles and policies to control access, as well as encryption of data at rest and in transit to protect our sensitive information.
        \end{enumerate}
    
\end{itemize}

This infrastructure is pivotal to our project, allowing us to effectively utilize big data technologies to derive valuable insights from the Formula 1 dataset. The design prioritizes scalability, efficiency, and cost-effectiveness, ensuring it meets the high demands of large-scale data processing and analysis. Ultimately, this setup empowers us to make data-driven decisions that enhance our understanding and strategic planning within the context of Formula 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/arch/AWS-GridGuru-LakeHouseArchitecture.png}
    \caption{\textbf{AWS Data LakeHouse Architecture Diagram}}
\end{figure}

% MISSING \section{Data Analysis with F1 Datasets} % MISSING 




\section{Challenges Encountered}
\subsection{Infrastructure-Related Challenges}
\paragraph{Description:}
Initially, our project was set up on Google Cloud Platform (GCP). However, we faced a significant challenge when our GCP credits were exhausted. To continue our work without interruption, we opted to create a simplified version of a Data Lakehouse on AWS. This decision was influenced by the opportunities and services available in AWS, which allowed us to replicate the necessary functionalities we initially had on GCP. We carefully selected AWS services that closely matched those we used on GCP to ensure a smooth transition and maintain the efficiency of our data processing and analysis.

% MISSING  \section{Conclusions} % MISSING 


\section{Project Repository}
\url{https://github.com/enriquegomeztagle/BigData/tree/main/FinalTerm/F1-GridGuru-Project}

\label{sec:references}
\begin{thebibliography}{9}
% \bibitem{ref1}

\end{thebibliography}

\end{document}
